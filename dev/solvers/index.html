<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Solvers · StructuredOptimization</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">StructuredOptimization</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../tutorial/">Quick Tutorial Guide</a></li><li><a class="tocitem" href="../expressions/">Expressions</a></li><li><a class="tocitem" href="../functions/">Functions</a></li><li class="is-active"><a class="tocitem" href>Solvers</a><ul class="internal"><li><a class="tocitem" href="#Minimizing-a-function-1"><span>Minimizing a function</span></a></li><li><a class="tocitem" href="#Specifying-solver-and-options-1"><span>Specifying solver and options</span></a></li><li><a class="tocitem" href="#Parse-and-solve-1"><span>Parse and solve</span></a></li><li><a class="tocitem" href="#References-1"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../demos/">Demos</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Solvers</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Solvers</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/kul-forbes/StructuredOptimization.jl/blob/master/docs/src/solvers.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Solvers-1"><a class="docs-heading-anchor" href="#Solvers-1">Solvers</a><a class="docs-heading-anchor-permalink" href="#Solvers-1" title="Permalink"></a></h1><h2 id="Minimizing-a-function-1"><a class="docs-heading-anchor" href="#Minimizing-a-function-1">Minimizing a function</a><a class="docs-heading-anchor-permalink" href="#Minimizing-a-function-1" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="StructuredOptimization.@minimize" href="#StructuredOptimization.@minimize"><code>StructuredOptimization.@minimize</code></a> — <span class="docstring-category">Macro</span></header><section><div><pre><code class="language-julia">@minimize cost [st ctr] [with slv_opt]</code></pre><p>Minimize a given problem with cost function <code>cost</code>, constraints <code>ctr</code> and solver options <code>slv_opt</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; using StructuredOptimization

julia&gt; A, b, x = randn(10,4), randn(10), Variable(4);

julia&gt; @minimize ls(A*x-b) + 0.5*norm(x);

julia&gt; ~x  # access array with solution

julia&gt; @minimize ls(A*x-b) st x &gt;= 0.;

julia&gt; ~x  # access array with solution

julia&gt; @minimize ls(A*x-b) st norm(x) == 2.0 with ForwardBackward(fast=true);

julia&gt; ~x  # access array with solution</code></pre><p>Returns as output a tuple containing the optimization variables and the number of iterations spent by the solver algorithm.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kul-forbes/StructuredOptimization.jl/blob/2207d5f879f11107821d0e4440d0ac4d9dbf01d9/src/solvers/minimize.jl#L3-L30">source</a></section></article><div class="admonition is-info"><header class="admonition-header">Problem warm-starting</header><div class="admonition-body"><p>By default <em>warm-starting</em> is always enabled. For example, if two problems that involve the same variables are solved consecutively, the second one will be automatically warm-started by the solution of the first one. That is because the variables are always linked to their respective data vectors. If one wants to avoid this, the optimization variables needs to be manually re-initialized before solving the second problem e.g. to a vector of zeros: <code>~x .= 0.0</code>.</p></div></div><h2 id="Specifying-solver-and-options-1"><a class="docs-heading-anchor" href="#Specifying-solver-and-options-1">Specifying solver and options</a><a class="docs-heading-anchor-permalink" href="#Specifying-solver-and-options-1" title="Permalink"></a></h2><p>You can pick the algorithm to use as <code>Solver</code> object from the <a href="https://github.com/kul-forbes/ProximalAlgorithms.jl"><code>ProximalAlgorithms.jl</code></a> package. Currently, the following algorithms are supported.</p><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.ForwardBackward" href="#ProximalAlgorithms.ForwardBackward"><code>ProximalAlgorithms.ForwardBackward</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ForwardBackward([gamma, adaptive, fast, maxit, tol, verbose, freq])</code></pre><p>Instantiate the Forward-Backward splitting algorithm (see [1, 2]) for solving optimization problems of the form</p><pre><code class="language-none">minimize f(Ax) + g(x),</code></pre><p>where <code>f</code> is smooth and <code>A</code> is a linear mapping (for example, a matrix). If <code>solver = ForwardBackward(args...)</code>, then the above problem is solved with</p><pre><code class="language-none">solver(x0, [f, A, g, L])</code></pre><p>Optional keyword arguments:</p><ul><li><code>gamma::Real</code> (default: <code>nothing</code>), the stepsize to use; defaults to <code>1/L</code> if not set (but <code>L</code> is).</li><li><code>adaptive::Bool</code> (default: <code>false</code>), if true, forces the method stepsize to be adaptively adjusted.</li><li><code>fast::Bool</code> (default: <code>false</code>), if true, uses Nesterov acceleration.</li><li><code>maxit::Integer</code> (default: <code>10000</code>), maximum number of iterations to perform.</li><li><code>tol::Real</code> (default: <code>1e-8</code>), absolute tolerance on the fixed-point residual.</li><li><code>verbose::Bool</code> (default: <code>true</code>), whether or not to print information during the iterations.</li><li><code>freq::Integer</code> (default: <code>10</code>), frequency of verbosity.</li></ul><p>If <code>gamma</code> is not specified at construction time, the following keyword argument can be used to set the stepsize parameter:</p><ul><li><code>L::Real</code> (default: <code>nothing</code>), the Lipschitz constant of the gradient of x ↦ f(Ax).</li></ul><p>References:</p><p>[1] Tseng, &quot;On Accelerated Proximal Gradient Methods for Convex-Concave Optimization&quot; (2008).</p><p>[2] Beck, Teboulle, &quot;A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems&quot;, SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183-202 (2009).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.ZeroFPR" href="#ProximalAlgorithms.ZeroFPR"><code>ProximalAlgorithms.ZeroFPR</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ZeroFPR([gamma, adaptive, memory, maxit, tol, verbose, freq, alpha, beta])</code></pre><p>Instantiate the ZeroFPR algorithm (see [1]) for solving optimization problems of the form</p><pre><code class="language-none">minimize f(Ax) + g(x),</code></pre><p>where <code>f</code> is smooth and <code>A</code> is a linear mapping (for example, a matrix). If <code>solver = ZeroFPR(args...)</code>, then the above problem is solved with</p><pre><code class="language-none">solver(x0, [f, A, g, L])</code></pre><p>Optional keyword arguments:</p><ul><li><code>gamma::Real</code> (default: <code>nothing</code>), the stepsize to use; defaults to <code>alpha/L</code> if not set (but <code>L</code> is).</li><li><code>adaptive::Bool</code> (default: <code>false</code>), if true, forces the method stepsize to be adaptively adjusted even if <code>L</code> is provided (this behaviour is always enforced if <code>L</code> is not provided).</li><li><code>memory::Integer</code> (default: <code>5</code>), memory parameter for L-BFGS.</li><li><code>maxit::Integer</code> (default: <code>1000</code>), maximum number of iterations to perform.</li><li><code>tol::Real</code> (default: <code>1e-8</code>), absolute tolerance on the fixed-point residual.</li><li><code>verbose::Bool</code> (default: <code>true</code>), whether or not to print information during the iterations.</li><li><code>freq::Integer</code> (default: <code>10</code>), frequency of verbosity.</li><li><code>alpha::Real</code> (default: <code>0.95</code>), stepsize to inverse-Lipschitz-constant ratio; should be in (0, 1).</li><li><code>beta::Real</code> (default: <code>0.5</code>), sufficient decrease parameter; should be in (0, 1).</li></ul><p>If <code>gamma</code> is not specified at construction time, the following keyword argument can be used to set the stepsize parameter:</p><ul><li><code>L::Real</code> (default: <code>nothing</code>), the Lipschitz constant of the gradient of x ↦ f(Ax).</li></ul><p>References:</p><p>[1] Themelis, Stella, Patrinos, &quot;Forward-backward envelope for the sum of two nonconvex functions: Further properties and nonmonotone line-search algorithms&quot;, SIAM Journal on Optimization, vol. 28, no. 3, pp. 2274–2303 (2018).</p></div></section></article><article class="docstring"><header><a class="docstring-binding" id="ProximalAlgorithms.PANOC" href="#ProximalAlgorithms.PANOC"><code>ProximalAlgorithms.PANOC</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PANOC([gamma, adaptive, memory, maxit, tol, verbose, freq, alpha, beta])</code></pre><p>Instantiate the PANOC algorithm (see [1]) for solving optimization problems of the form</p><pre><code class="language-none">minimize f(Ax) + g(x),</code></pre><p>where <code>f</code> is smooth and <code>A</code> is a linear mapping (for example, a matrix). If <code>solver = PANOC(args...)</code>, then the above problem is solved with</p><pre><code class="language-none">solver(x0, [f, A, g, L])</code></pre><p>Optional keyword arguments:</p><ul><li><code>gamma::Real</code> (default: <code>nothing</code>), the stepsize to use; defaults to <code>alpha/L</code> if not set (but <code>L</code> is).</li><li><code>adaptive::Bool</code> (default: <code>false</code>), if true, forces the method stepsize to be adaptively adjusted even if <code>L</code> is provided (this behaviour is always enforced if <code>L</code> is not provided).</li><li><code>memory::Integer</code> (default: <code>5</code>), memory parameter for L-BFGS.</li><li><code>maxit::Integer</code> (default: <code>1000</code>), maximum number of iterations to perform.</li><li><code>tol::Real</code> (default: <code>1e-8</code>), absolute tolerance on the fixed-point residual.</li><li><code>verbose::Bool</code> (default: <code>true</code>), whether or not to print information during the iterations.</li><li><code>freq::Integer</code> (default: <code>10</code>), frequency of verbosity.</li><li><code>alpha::Real</code> (default: <code>0.95</code>), stepsize to inverse-Lipschitz-constant ratio; should be in (0, 1).</li><li><code>beta::Real</code> (default: <code>0.5</code>), sufficient decrease parameter; should be in (0, 1).</li></ul><p>If <code>gamma</code> is not specified at construction time, the following keyword argument can be used to set the stepsize parameter:</p><ul><li><code>L::Real</code> (default: <code>nothing</code>), the Lipschitz constant of the gradient of x ↦ f(Ax).</li></ul><p>References:</p><p>[1] Stella, Themelis, Sopasakis, Patrinos, &quot;A simple and efficient algorithm for nonlinear model predictive control&quot;, 56th IEEE Conference on Decision and Control (2017).</p></div></section></article><h2 id="Parse-and-solve-1"><a class="docs-heading-anchor" href="#Parse-and-solve-1">Parse and solve</a><a class="docs-heading-anchor-permalink" href="#Parse-and-solve-1" title="Permalink"></a></h2><p>The macro <a href="#StructuredOptimization.@minimize"><code>@minimize</code></a> automatically parse and solve the problem. An alternative syntax is given by the function <a href="#StructuredOptimization.problem"><code>problem</code></a> and <a href="#StructuredOptimization.solve"><code>solve</code></a>.</p><article class="docstring"><header><a class="docstring-binding" id="StructuredOptimization.problem" href="#StructuredOptimization.problem"><code>StructuredOptimization.problem</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">problems(terms...)</code></pre><p>Constructs a problem.</p><p><strong>Example</strong></p><pre><code class="language-julia">
julia&gt; x = Variable(4)
Variable(Float64, (4,))

julia&gt; A, b = randn(10,4), randn(10);

julia&gt; p = problem(ls(A*x-b), norm(x) &lt;= 1)
</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kul-forbes/StructuredOptimization.jl/blob/2207d5f879f11107821d0e4440d0ac4d9dbf01d9/src/syntax/problem.jl#L3-L21">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="StructuredOptimization.solve" href="#StructuredOptimization.solve"><code>StructuredOptimization.solve</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">solve(terms::Tuple, solver::ForwardBackwardSolver)</code></pre><p>Takes as input a tuple containing the terms defining the problem and the solver options.</p><p>Solves the problem returning a tuple containing the iterations taken and the build solver.</p><p><strong>Example</strong></p><pre><code class="language-julia">julia&gt; x = Variable(4)
Variable(Float64, (4,))

julia&gt; A, b = randn(10,4), randn(10);

julia&gt; p = problem(ls(A*x - b ), norm(x) &lt;= 1);

julia&gt; solve(p, ForwardBackward());

julia&gt; ~x</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/kul-forbes/StructuredOptimization.jl/blob/2207d5f879f11107821d0e4440d0ac4d9dbf01d9/src/solvers/build_solve.jl#L51-L72">source</a></section></article><p>Once again, the <code>Solver</code> objects is to be picked from <a href="https://github.com/kul-forbes/ProximalAlgorithms.jl"><code>ProximalAlgorithms.jl</code></a>).</p><h2 id="References-1"><a class="docs-heading-anchor" href="#References-1">References</a><a class="docs-heading-anchor-permalink" href="#References-1" title="Permalink"></a></h2><p><a href="http://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf">[1]</a> Tseng, <em>On Accelerated Proximal Gradient Methods for Convex-Concave Optimization</em> (2008).</p><p><a href="http://epubs.siam.org/doi/abs/10.1137/080716542">[2]</a> Beck, Teboulle, <em>A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems</em>, SIAM Journal on Imaging Sciences, vol. 2, no. 1, pp. 183-202 (2009).</p><p><a href="https://arxiv.org/abs/1606.06256">[3]</a> Themelis, Stella, Patrinos, <em>Forward-backward envelope for the sum of two nonconvex functions: Further properties and nonmonotone line-search algorithms</em>, arXiv:1606.06256 (2016).</p><p><a href="https://doi.org/10.1109/CDC.2017.8263933">[4]</a> Stella, Themelis, Sopasakis, Patrinos, <em>A simple and efficient algorithm for nonlinear model predictive control</em>, 56th IEEE Conference on Decision and Control (2017).</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../functions/">« Functions</a><a class="docs-footer-nextpage" href="../demos/">Demos »</a></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 21 May 2021 11:28">Friday 21 May 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
