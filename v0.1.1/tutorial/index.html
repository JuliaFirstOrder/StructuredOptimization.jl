<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Quick Tutorial Guide · StructuredOptimization</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>StructuredOptimization</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" id="search-form" action="../search/"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../">Home</a></li><li class="current"><a class="toctext" href>Quick Tutorial Guide</a><ul class="internal"><li><a class="toctext" href="#Standard-problem-formulation-1">Standard problem formulation</a></li><li><a class="toctext" href="#Unconstrained-optimization-1">Unconstrained optimization</a></li><li><a class="toctext" href="#Constrained-optimization-1">Constrained optimization</a></li><li><a class="toctext" href="#Using-multiple-variables-1">Using multiple variables</a></li><li><a class="toctext" href="#Limitations-1">Limitations</a></li></ul></li><li><a class="toctext" href="../expressions/">Expressions</a></li><li><a class="toctext" href="../functions/">Functions</a></li><li><a class="toctext" href="../solvers/">Solvers</a></li><li><a class="toctext" href="../demos/">Demos</a></li></ul></nav><article id="docs"><header><nav><ul><li><a href>Quick Tutorial Guide</a></li></ul><a class="edit-page" href="https://github.com/kul-forbes/StructuredOptimization.jl/blob/master/docs/src/tutorial.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Quick Tutorial Guide</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Quick-tutorial-guide-1" href="#Quick-tutorial-guide-1">Quick tutorial guide</a></h1><h2><a class="nav-anchor" id="Standard-problem-formulation-1" href="#Standard-problem-formulation-1">Standard problem formulation</a></h2><p>Currently with StructuredOptimization.jl one can solve problems of the form</p><div>\[\underset{ \mathbf{x} }{\text{minimize}} \ f(\mathbf{x}) + g(\mathbf{x}),\]</div><p>where <span>$f$</span> is a smooth function while <span>$g$</span> is possibly nonsmooth.</p><h2><a class="nav-anchor" id="Unconstrained-optimization-1" href="#Unconstrained-optimization-1">Unconstrained optimization</a></h2><p>The <em>least absolute shrinkage and selection operator</em> (LASSO) belongs to this class of problems:</p><div>\[\underset{ \mathbf{x} }{\text{minimize}} \ \tfrac{1}{2} \| \mathbf{A} \mathbf{x} - \mathbf{y} \|^2+ \lambda \| \mathbf{x} \|_1.\]</div><p>Here the squared norm <span>$\tfrac{1}{2} \| \mathbf{A} \mathbf{x} - \mathbf{y} \|^2$</span> is a <em>smooth</em> function <span>$f$</span> whereas the <span>$l_1$</span>-norm is a <em>nonsmooth</em> function <span>$g$</span>. This problem can be solved with only few lines of code:</p><pre><code class="language-julia">julia&gt; using StructuredOptimization

julia&gt; n, m = 100, 10;                # define problem size

julia&gt; A, y = randn(m,n), randn(m);   # random problem data

julia&gt; x = Variable(n);               # initialize optimization variable

julia&gt; λ = 1e-2*norm(A&#39;*y,Inf);       # define λ    

julia&gt; @minimize ls( A*x - y ) + λ*norm(x, 1); # solve problem

julia&gt; ~x                             # inspect solution
100-element Array{Float64,1}:
  0.0
  0.0
  0.0
  0.440254
  0.0
  0.0
  0.0
[...]</code></pre><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>The function <code>ls</code> is a short hand notation for <code>0.5*norm(...)^2</code>, namely a least squares term.</p></div></div><p>It is possible to access to the solution by typing <code>~x</code>. By default variables are initialized by <code>Array</code>s of zeros. Different initializations can be set during construction <code>x = Variable( [1.; 0.; ...] )</code> or by assignment <code>~x .= [1.; 0.; ...]</code>.</p><h2><a class="nav-anchor" id="Constrained-optimization-1" href="#Constrained-optimization-1">Constrained optimization</a></h2><p>Constrained optimization is also encompassed by the <a href="#Standard-problem-formulation-1">Standard problem formulation</a>: for a nonempty set <span>$\mathcal{S}$</span> the constraint of</p><div>\[\begin{align*}
\underset{ \mathbf{x} }{\text{minimize}} \ &amp;  f(\mathbf{x}) \\
\text{subject to} \ &amp; \mathbf{x} \in \mathcal{S}
\end{align*}\]</div><p>can be converted into an <em>indicator function</em></p><div>\[g(\mathbf{x}) = \delta_{\mathcal{S}} (\mathbf{x}) =  \begin{cases}
    0       &amp; \text{if} \ \mathbf{x} \in \mathcal{S},\\
    +\infty &amp; \text{otherwise}.
    \end{cases}\]</div><p>Constraints are treated as <em>nonsmooth functions</em>. This conversion is automatically performed by StructuredOptimization.jl. For example, the non-negative deconvolution problem:</p><div>\[\begin{align*}
\underset{ \mathbf{x} }{\text{minimize}} \ &amp;  \tfrac{1}{2} \| \mathbf{x} * \mathbf{h} - \mathbf{y} \|^2 \\
\text{subject to} \ &amp; \mathbf{x} \geq 0
\end{align*}\]</div><p>where <span>$*$</span> stands for convolution and <span>$\mathbf{h}$</span> contains the taps of a finite impulse response filter, can be solved using the following lines of code:</p><pre><code class="language-julia">julia&gt; n = 10;                        # define problem size

julia&gt; x = Variable(n);               # define variable

julia&gt; h, y = randn(n), randn(2*n-1); # random filter taps and output

julia&gt; @minimize ls(conv(x,h)-y) st x &gt;= 0.
</code></pre><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>The convolution mapping was applied to the variable <code>x</code> using <code>conv</code>. StructuredOptimization.jl provides a set of functions that can be used to apply specific operators to variables and create mathematical expression. The available functions can be found in <a href="../expressions/#Mappings-1">Mappings</a>. In general it is more convenient to use these functions instead of matrices, as these functions apply efficient algorithms for the forward and adjoint mappings leading to <em>matrix free optimization</em>.</p></div></div><h2><a class="nav-anchor" id="Using-multiple-variables-1" href="#Using-multiple-variables-1">Using multiple variables</a></h2><p>It is possible to use multiple variables which are allowed to be matrices or even tensors. For example a non-negative matrix factorization problem:</p><div>\[\begin{align*}
\underset{ \mathbf{X}_1, \mathbf{X}_2  }{\text{minimize}} \ &amp;  \tfrac{1}{2} \| \mathbf{X}_1 \mathbf{X}_2 - \mathbf{Y} \| \\
\text{subject to} \ &amp; \mathbf{X}_1 \geq 0,  \ \mathbf{X}_2 \geq 0,
\end{align*}\]</div><p>can be solved using the following code:</p><pre><code class="language-julia"># matrix variables initialized with random coefficients
julia&gt; X1, X2 = Variable(rand(n,l)), Variable(rand(l,m));

julia&gt; Y = rand(n,m);

julia&gt; @minimize ls(X1*X2-Y) st X1 &gt;= 0., X2 &gt;= 0.
</code></pre><h2><a class="nav-anchor" id="Limitations-1" href="#Limitations-1">Limitations</a></h2><p>Currently StructuredOptimization.jl supports only <em>proximal gradient algorithms</em> (i.e., <em>forward-backward splitting</em> base), which require specific properties of the nonsmooth functions and constraint to be applicable. In particular, the nonsmooth functions must have an <em>efficiently computable proximal mapping</em>.</p><p>If we express the nonsmooth function <span>$g$</span> as the composition of a function <span>$\tilde{g}$</span> with a linear operator <span>$A$</span>:</p><div>\[g(\mathbf{x}) =
\tilde{g}(A \mathbf{x})\]</div><p>then the proximal mapping of <span>$g$</span> is efficiently computable if either of the following hold:</p><ol><li><p>Operator <span>$A$</span> is a <em>tight frame</em>, namely it satisfies <span>$A A^* = \mu Id$</span>, where <span>$\mu \geq 0$</span>, <span>$A^*$</span> is the adjoint of <span>$A$</span>, and <span>$Id$</span> is the identity operator.</p></li><li><p>Function <span>$g$</span> is a <em>separable sum</em> <span>$g(\mathbf{x}) = \sum_j h_j (B_j \mathbf{x}_j)$</span>, where <span>$\mathbf{x}_j$</span> are non-overlapping slices of <span>$\mathbf{x}$</span>, and <span>$B_j$</span> are tight frames.</p></li></ol><p>Let us analyze these rules with a series of examples. The LASSO example above satisfy the first rule:</p><pre><code class="language-julia">julia&gt; @minimize ls( A*x - y ) + λ*norm(x, 1)</code></pre><p>since the nonsmooth function <span>$\lambda \| \cdot \|_1$</span> is not composed with any operator (or equivalently is composed with <span>$Id$</span> which is a tight frame). Also the following problem would be accepted by StructuredOptimization.jl:</p><pre><code class="language-julia">julia&gt; @minimize ls( A*x - y ) + λ*norm(dct(x), 1)</code></pre><p>since the discrete cosine transform (DCT) is orthogonal and is therefore a tight frame. On the other hand, the following problem</p><pre><code class="language-julia">julia&gt; @minimize ls( A*x - y ) + λ*norm(x, 1) st x &gt;= 1.0</code></pre><p>cannot be solved through proximal gradient algorithms, since the second rule would be violated. Here the constraint would be converted into an indicator function and the nonsmooth function <span>$g$</span> can be written as the sum:</p><div>\[g(\mathbf{x}) =\lambda \| \mathbf{x} \|_1 + \delta_{\mathcal{S}} (\mathbf{x})\]</div><p>which is not separable. On the other hand this problem would be accepted:</p><pre><code class="language-julia">julia&gt; @minimize ls( A*x - y ) + λ*norm(x[1:div(n,2)], 1) st x[div(n,2)+1:n] &gt;= 1.0</code></pre><p>as not the optimization variables <span>$\mathbf{x}$</span> are partitioned into non-overlapping groups.</p><div class="admonition note"><div class="admonition-title">Note</div><div class="admonition-text"><p>When the problem is not accepted it might be still possible to solve it: see <a href="../functions/#Smoothing-1">Smoothing</a> and <a href="../functions/#Duality-1">Duality</a>.</p></div></div><footer><hr/><a class="previous" href="../"><span class="direction">Previous</span><span class="title">Home</span></a><a class="next" href="../expressions/"><span class="direction">Next</span><span class="title">Expressions</span></a></footer></article></body></html>
