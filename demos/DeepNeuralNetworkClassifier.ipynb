{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Deep Neural Network Classifier\n",
    "\n",
    "\n",
    "A _deep neural network_ (DNN) is a relatively simple model of brain neurons that, by composing\n",
    "multiple linear transformations and nonlinear _activation functions_,\n",
    "allows obtaining highly nonlinear mappings that can be used for classification\n",
    "or regression tasks.\n",
    "\n",
    "Suppose there are two sets of data points $\\mathcal{A}$ and $\\mathcal{B}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg, Random, Statistics, LinearAlgebra\n",
    "\n",
    "Pkg.activate(\".\"); Pkg.instantiate(); Random.seed!(11) # For reproducibility\n",
    "\n",
    "# construct dataset\n",
    "Na, Nb = 800,800 # number of data points of set A and B\n",
    "N = Na+Nb\n",
    "rhoa = 5.\n",
    "\n",
    "A = hcat([ rhoa.*[ cos(theta);  sin(theta)] for theta in range(0,2*pi;length=Na) ]...)\n",
    "A .+= 1.5.*randn(size(A))\n",
    "B = zeros(2, div(Nb,2))\n",
    "rhob = 8.\n",
    "B = [B hcat([ rhob.*[ cos(theta);  sin(theta)] for theta in range(0,2*pi;length=div(Nb,2)) ]...)]\n",
    "B .+= randn(size(B))\n",
    "\n",
    "D = [A B] # data matrix \n",
    "yt = [ones(Bool,Na); zeros(Bool,Nb)] # labels: 1 is set A, 0 is set B\n",
    "\n",
    "# preprocess data\n",
    "D .-= mean(D)\n",
    "D ./= sqrt.(var(D,dims=2))\n",
    "\n",
    "using Plots\n",
    "scatter(A[1,:],A[2,:]; m=:o, label  = \"A\")\n",
    "scatter!(B[1,:],B[2,:]; m=:star, label  = \"B\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A classifier seeks to find the lines that separate these sets. Clearly, a linear classifier cannot perform such task. On the contrary, a DNN with its nonlinear behavior can be _trained_ to perform such task. \n",
    "\n",
    "Firstly the DNN has to be constructed. Here a three layer configuration is used:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{y} &= \\mathcal{S} ( \\mathbf{W}_3 \\mathbf{L}_2 +b_3 ) \\\\\n",
    "\\mathbf{L}_2 &= \\mathcal{S} ( \\mathbf{W}_2 \\mathbf{L}_1 +b_2 ) \\\\\n",
    "\\mathbf{L}_1 &= \\mathcal{S} ( \\mathbf{W}_1 \\mathbf{D} + b_1 ), \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{S}$ are sigmoid functions modeling the activations of the neurons, $\\mathbf{W}_i$ are the _weights_ of the connections between the neighbor layers neurons, $b_i$ are the biases of the layers, $\\mathbf{D}$ are the training data points and $\\mathbf{y}$ is the output of the DNN. \n",
    "\n",
    "This DNN can be constructed using `StructuredOptimization` together with `AbstractOperators`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using AbstractOperators, StructuredOptimization\n",
    "\n",
    "n = 7  # inner layers nodes\n",
    "\n",
    "dim_in, N = size(D)\n",
    "\n",
    "# weights are initialized with random variables normalized \n",
    "# by the inverse square root of the input output dimensions \n",
    "\n",
    "W1 = Variable(sqrt(2/(2+n)*N)*randn(n,dim_in)) # first layer\n",
    "S1 = Sigmoid((n,N))\n",
    "b1 = Variable(1)\n",
    "\n",
    "W2 = Variable(sqrt(2/(2*n))*randn(n,n))        # second layer\n",
    "S2 = Sigmoid((n,N))\n",
    "b2 = Variable(1)\n",
    "\n",
    "W3 = Variable(sqrt(2/(n+1))*randn(1,n))        # third layer\n",
    "S3 = Sigmoid((1,N))\n",
    "b3 = Variable(1)\n",
    "\n",
    "L1 = S1*(W1* D.+b1)\n",
    "L2 = S2*(W2*L1.+b2)\n",
    "y  = S3*(W3*L2.+b3);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training is performed by matching the output of the layer $\\mathbf{y}$ with the labels $\\tilde{\\mathbf{y}}$ by finding the optimal weights $\\mathbf{W}_i$ and biases $b_i$. \n",
    "\n",
    "This can be achieved by solving the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{W}_{1},\\mathbf{W}_{2},\\mathbf{W}_{3}, b_{1},b_{2},b_{3} }{\\text{minimize}} -\\sum_{n = 1}^N \\overbrace{\\left(\\tilde{y}_n \\log (y_n) +(1-\\tilde{y}_n) \\log (1-y_n) \\right)}^{f}+ \\sum_{i = 1}^3 \\overbrace{ \\lambda_i \\| \\text{vec} (\\mathbf{W}_i)\\|_{1}}^{g}\n",
    "$$\n",
    "\n",
    "where $f$ is the _cross entropy loss_ function and $g$ is a regularization function which prevents _over-fitting_ and promotes enforcing the weights $\\mathbf{W}_i$ to be sparse matrices. \n",
    "\n",
    "The parameters $\\lambda_i$ can be found using a K-fold cross validation strategy which is not reported here for brevity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda = 0.0031622776601683794 # lambda obtained from K-fold CV\n",
    "\n",
    "lambda1 = lambda/(2*n)\n",
    "lambda2 = lambda/(n+n)\n",
    "lambda3 = lambda/n\n",
    "\n",
    "g = lambda1*norm(W1,1)+lambda2*norm(W2,1)+lambda3*norm(W3,1)\n",
    "\n",
    "@minimize crossentropy(y,yt) + g with PANOC(tol = 1e-4, verbose=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that the weights and biases are obtained it is possible to check the regions that are identified by the trained DNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(-3,3;length=200) # define grid of points\n",
    "yy = range(-3,3;length=200)\n",
    "\n",
    "R = vcat([[xi yi] for yi in yy, xi in xx][:]...)' # new inputs to trained DNN\n",
    "S = Sigmoid((n,size(R,2)))\n",
    "Sout = Sigmoid((1,size(R,2)))\n",
    "\n",
    "L1 = S*(~W1* R .+ ~b1)\n",
    "L2 = S*(~W2*L1 .+ ~b2)\n",
    "r  = Sout*(~W3*L2 .+ ~b3) # new DNN output\n",
    "\n",
    "r = reshape(r,200,200)   \n",
    "scatter(D[1,Na+1:end],D[2,Na+1:end]; m=:o, label = \"A\")\n",
    "scatter!(D[1,1:Na],D[2,1:Na]; m=:star, label = \"B\")\n",
    "contour!(xx,yy,r; levels = 0.7*[maximum(r)], linewidths = 5);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.1.1",
   "language": "julia",
   "name": "julia-1.1"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
